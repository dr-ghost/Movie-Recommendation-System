# -*- coding: utf-8 -*-
"""PRMLMajor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l5qbI7N9lUKJIIf9pwmVneiKNtYxM8kv
"""

def exception_handler(func):
    def inner_function(*args, **kwargs):
        try:
            func(*args, **kwargs)
        except TypeError:
            print(f"{func.__name__} only takes numbers as the argument")
        except ModuleNotFoundError:
            print("Error not included all libraries")
    return inner_function

from google.colab import drive
drive.mount('/content/drive')

"""Implementation #1
Item Based Collaborative Filtering
"""

import pandas as pd
import numpy as np
import re

from datetime import datetime

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn import metrics, preprocessing

from tensorflow.keras import models, layers, utils

df_movies = pd.read_csv("/content/drive/MyDrive/data/movies.csv")

df_movies

df_users = pd.read_csv("/content/drive/MyDrive/data/ratings.csv")

df_users

"""Data Preprocessing"""

df_movies = df_movies.loc[:,["movieId","title"]]

df_movies

df_users = df_users.loc[:,["userId","movieId","rating"]]

df_users

data = pd.merge(df_movies, df_users)

data

data = data.iloc[:5000000,:]

df_data = data.pivot_table(index = ["userId"],columns = ["title"],values = "rating")

df_data

df_data.shape

df_data.index

def return_item_based_recomended(movie_w : str, n_items = None) -> pd.DataFrame:
    movie = df_data[movie_w]
    similar_movies = df_data.corrwith(movie)
    print(similar_movies)
    similar_movies = similar_movies.sort_values(ascending=False)

    if (n_items == None):
        return similar_movies
    else:
        return similar_movies.iloc[:n_items]

movie01 = return_item_based_recomended("Toy Story (1995)", 10)

movie01

movie02 = return_item_based_recomended("Waiting to Exhale (1995)", 15)

movie02

print(df_data.index)

"""Implementation #2"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn import metrics, preprocessing
from datetime import datetime

import re
import os
from scipy.sparse import csr_matrix
from sklearn.neighbors import NearestNeighbors

class RecommendationSystem2(object):
    @exception_handler
    def __init__(self, df_ratings, df_movies):
        self.df_ratings = df_ratings.loc[:1000000,:]
        self.df_movies = df_movies

        self.data = pd.crosstab(index=self.df_ratings['movieId'], columns=self.df_ratings['userId'], values=self.df_ratings['rating'], aggfunc='first')
        self.data.fillna(0,inplace=True)

    def train(self, n_neighbours = 10):
        from scipy.sparse import csr_matrix

        from sklearn.neighbors import KNeighborsClassifier
        from sklearn.neighbors import NearestNeighbors

        self.csr_data = csr_matrix(self.data.values)
        self.data.reset_index(inplace=True)

        self.model = NearestNeighbors(n_neighbors = n_neighbours)
        self.model.fit(self.csr_data)

    def recommend(self, movie : str, n_movies = 10):
        inp = self.df_movies[self.df_movies['title'].str.contains(movie)]
        self.data.reset_index(inplace=True)
        if len(inp) == 0:
            raise TypeError()

        index_mov = self.data[self.data['movieId'] == inp.iloc[0]['movieId']].index[0]

        self.train(n_movies)

        _, res = self.model.kneighbors(self.csr_data[index_mov])

        res = res[0][1:]

        recomended_movies = []
        for _, id in enumerate(res):
            recomended_movies.append(self.df_movies[self.df_movies['movieId'] == self.data.iloc[id]['movieId']]['title'].values[0])

        return recomended_movies

#Testing Model 2

df_rating = pd.read_csv('/content/drive/MyDrive/data/ratings.csv')

df_rating

df_movie = pd.read_csv('/content/drive/MyDrive/data/movies.csv')

df_movie

RS2 = RecommendationSystem2(df_rating, df_movie)

RS2.recommend('Toy Story')

RS2_ = RecommendationSystem2(df_rating, df_movie)

RS2_.recommend('Father of the Bride')

"""Implementation 3
User based agglomerative clustering
"""

#Model 3 : User based agglomerative clustering
ratings_user = df_data
ratings_user = ratings_user.dropna(thresh = 3,axis = 1).fillna(0)
ratings_user = ratings_user[:10000]
ratings_user.shape

mat1 = ratings_user.to_numpy()
print(mat1.shape)
mat2 = np.dot(mat1,mat1.T)

print(len(df_data.index))

from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram
from sklearn.decomposition import PCA

def plot_dendrogram(model, **kwargs):

    # Create linkage matrix and then plot the dendrogram
    # create the counts of samples under each node

    counts = np.zeros(model.children_.shape[0])
    n_samples = len(model.labels_)

    for i, merge in enumerate(model.children_):
        current_count = 0
        for child_idx in merge:
            if child_idx < n_samples:
                current_count += 1  # leaf node
            else:
                current_count += counts[child_idx - n_samples]
        counts[i] = current_count

    linkage_matrix = np.column_stack( [model.children_, model.distances_, counts] ).astype(float)

    # Plot the corresponding dendrogram
    dendrogram(linkage_matrix, **kwargs)

new_dvt = ratings_user
model = AgglomerativeClustering(distance_threshold=0, n_clusters=None, linkage='ward').fit(new_dvt)

plot_dendrogram(model, truncate_mode="level", p=4)
#new_dvt.head()

df = new_dvt
model_new = AgglomerativeClustering(n_clusters=3,linkage='ward').fit(df)
pred = model_new.fit_predict(df)
clus = np.array(pred)
clus = clus.reshape(-1,1)
columns = list(df.columns)

df['Clus'] = clus
df.head()

out = {}
for i in range(3):

  clust = df.loc[df['Clus'] == i]
  print(clust.to_numpy().shape)

  out[i] = clust.drop(['Clus'],axis = 1)

movie_names = list(ratings_user.columns)
print(movie_names)

data_clus = out
print(data_clus)

def user_encode(movie_lis,rating_lis):
  out = np.zeros((725,))
  #print(out.shape)
  for i in range (len(movie_lis)):
    if(movie_lis[i] in movie_names):
      ind = movie_names.index(movie_lis[i])
      out[ind] = rating_lis[i]

  return out

user = user_encode(['2 days in the valley', '8 Seconds (1994)'],[3, 4])

user.shape

centroid_dict = {}
for i in range(3):
  data = out[i]
  centroid = np.mean(data,axis = 0)
  centroid_dict[i] = np.array(centroid)
  #print(np.array(centroid))

centroid_dict

def get_data(centroids,user,data_clus,clus_count):
  listi = []
  for i in range(clus_count):
     centroid = centroids[i]
     dist = np.sum((user - centroid)**2)
     listi.append(dist)

  cluster = listi.index(min(listi))

  return data_clus[cluster]

print(get_data(centroid_dict,user,data_clus,3))

def return_item_based_recomended(centroids, user, data_clus, clus_count, movie_w : str, n_items = None) -> pd.DataFrame:
    global df_data

    df_tmp = pd.DataFrame(get_data(centroids,user,data_clus,clus_count), columns = movie_names)
    tmp_lst = list(df_tmp.index)
    print(df_data.index)
    print(tmp_lst)
    df_data = df_data.loc[tmp_lst, :]
    movie = df_data[movie_w]
    similar_movies = df_data.corrwith(movie)
    similar_movies = similar_movies.sort_values(ascending=False)

    if (n_items == None):
        return similar_movies
    else:
        return similar_movies.iloc[:n_items]

return_item_based_recomended(centroid_dict,user,data_clus, 3,"8 Seconds (1994)", 3)

